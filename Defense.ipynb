{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42) \n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "from keras import backend as K\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "scaler = MinMaxScaler()\n",
    "categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./dataset/Data_defense/data_defense_defender1.csv')\n",
    "data_set_defense_adv=data.loc[data['label'] == 1]\n",
    "data_set_defense_clean=data.loc[data['label'] == 0]\n",
    "\n",
    "train_defender_adv, test_defender_adv = train_test_split(data_set_defense_adv,shuffle=False, test_size=0.25)\n",
    "train_defender_clean, test_defender_clean= train_test_split(data_set_defense_clean, shuffle=False,test_size=0.25)\n",
    "train_defender=pd.concat([train_defender_adv, train_defender_clean], ignore_index = True)\n",
    "test_defender=pd.concat([test_defender_adv, test_defender_clean], ignore_index = True)\n",
    "\n",
    "X_train_defender = train_defender.drop(columns = [\"label\"])\n",
    "y_train_defender= train_defender.label\n",
    "X_test_defender= test_defender.drop(columns = [\"label\"])\n",
    "y_test_defender= test_defender.label\n",
    "\n",
    "\n",
    "# Normalization\n",
    "scaler3 = MinMaxScaler()\n",
    "scaler3.fit(X_train_defender.to_numpy())\n",
    "X_train_defender_scaled = scaler3.transform(X_train_defender)\n",
    "X_test_defender_scaled = scaler3.transform(X_test_defender)\n",
    " \n",
    "# Trained  dataset\n",
    "dport_train = X_train_defender[['dportWell-known', 'dportUnspecified', 'dportRegistered', 'dportDynamic/Private']]\n",
    "sport_train = X_train_defender[['sportWell-known', 'sportUnspecified',\n",
    "    'sportRegistered', 'sportDynamic/Private']]\n",
    "proto_train = X_train_defender[['protoudp','prototcp', 'protorarp',\n",
    "    'protoigmp', 'protoicmp', 'protoarp']]\n",
    "rest_train = ['dur', 'pkts', 'bytes', 'spkts',\n",
    "    'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate']\n",
    "\n",
    "data_train=[]\n",
    "data_train.append(dport_train)\n",
    "data_train.append(sport_train)\n",
    "data_train.append(proto_train)\n",
    "for i in range(0,len(rest_train)):\n",
    "    d = X_train_defender[[rest_train[i]]]\n",
    "    data_train.append(d)\n",
    "    \n",
    "# Evaluation dataset\n",
    "dport_test = X_test_defender[['dportWell-known', 'dportUnspecified', 'dportRegistered', 'dportDynamic/Private']]\n",
    "sport_test = X_test_defender[['sportWell-known', 'sportUnspecified',\n",
    "    'sportRegistered', 'sportDynamic/Private']]\n",
    "proto_test = X_test_defender[['protoudp','prototcp', 'protorarp',\n",
    "    'protoigmp', 'protoicmp', 'protoarp']]\n",
    "rest_test = ['dur', 'pkts', 'bytes', 'spkts',\n",
    "    'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate']\n",
    "\n",
    "data_test=[]\n",
    "data_test.append(dport_test)\n",
    "data_test.append(sport_test)\n",
    "data_test.append(proto_test)\n",
    "for i in range(0,len(rest_test)):\n",
    "    d = X_test_defender[[rest_test[i]]]\n",
    "    data_test.append(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\condor\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Clusters Training \n",
    "all_cluster = []\n",
    "factor_of_reliability = []\n",
    "for features in range(0,len(data_test)):  \n",
    "\n",
    "    cluster =RandomForestClassifier(n_estimators = 200, n_jobs = -1, random_state=0)\n",
    "    cluster.fit(data_train[features], y_train_defender)\n",
    "    # for DNN\n",
    "    '''scaler.fit(data_train[features].to_numpy())\n",
    "    data_train_scaled = scaler.transform(data_train[features])\n",
    "    data_test_scaled = scaler.transform(data_test[features]) \n",
    "    \n",
    "    y_train_defender_ohe = pd.get_dummies(y_train_defender)\n",
    "    y_train_defender_ohe_tf = tf.convert_to_tensor(y_train_defender_ohe, np.float32)\n",
    "    y_test_defender_ohe = pd.get_dummies(y_test_defender)\n",
    "    y_test_defender_ohe_tf = tf.convert_to_tensor(y_test_defender_ohe, np.float32)\n",
    "\n",
    "    output_number = 2\n",
    "    eval_metric = 'categorical_accuracy'\n",
    "    activ_out = 'softmax'\n",
    "    #activ_out = 'relu'\n",
    "    neurons_number = 256\n",
    "    lr = 0.01\n",
    "    features_number = data_train_scaled.shape[1]\n",
    "    cluster = tf.keras.Sequential([tf.keras.layers.Dense(neurons_number, input_shape=(features_number,), activation=\"relu\"), tf.keras.layers.Dense(neurons_number, activation=\"relu\"), tf.keras.layers.Dense(output_number, activation=activ_out)])\n",
    "    cluster.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss = categorical_cross_entropy, metrics=[eval_metric])\n",
    "    cluster.fit(x=data_train_scaled, y=y_train_defender_ohe_tf, epochs=10, batch_size=100, verbose=1)\n",
    "    cluster.evaluate(x=data_test_scaled, y=y_test_defender_ohe_tf, verbose=1)''' \n",
    "          \n",
    "    pred = cluster.predict(data_test[features])\n",
    "    matrix= classification_report(output_dict = True,y_true=y_test_defender, y_pred=pred)\n",
    "    #matrix= classification_report(output_dict = True,y_true=y_test_defender, y_pred=y_pred_vect) #For DNN \n",
    "    factor_of_reliability.append(round(matrix['0']['recall'],2))\n",
    "    all_cluster.append(cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "data_attaque=pd.read_csv('./dataset/Data_defense/data_defense_attacker.csv')\n",
    "data_attaque=data_attaque.drop(columns = ['category'])\n",
    "X_test_attaque= data_attaque.drop(columns = [\"label\"])\n",
    "y_test_attaque= data_attaque.label\n",
    "\n",
    "dport_test = X_test_defender[['dportWell-known', 'dportUnspecified', 'dportRegistered', 'dportDynamic/Private']]\n",
    "sport_test = X_test_defender[['sportWell-known', 'sportUnspecified',\n",
    "    'sportRegistered', 'sportDynamic/Private']]\n",
    "proto_test = X_test_defender[['protoudp','prototcp', 'protorarp',\n",
    "    'protoigmp', 'protoicmp', 'protoarp']]\n",
    "rest_test = ['dur', 'pkts', 'bytes', 'spkts',\n",
    "    'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate']\n",
    "\n",
    "data_test=[]\n",
    "data_test.append(dport_test)\n",
    "data_test.append(sport_test)\n",
    "data_test.append(proto_test)\n",
    "for i in range(0,len(rest_test)):\n",
    "    d = X_test_defender[[rest_test[i]]]\n",
    "    data_test.append(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pred = []\n",
    "i = 0\n",
    "for cluster in all_cluster:\n",
    "    # Multiplication of probs by their defensive weights\n",
    "    X_defender_scaled_cluster = data_test[i]\n",
    "    # For DNN\n",
    "    '''scaler.fit(data_test[i].to_numpy())\n",
    "    X_defender_scaled_cluster = scaler.transform(data_test[i])\n",
    "    def predict_prob(number):\n",
    "        return [number[0],1-number[0]]\n",
    "    y_pred = np.array(list(map(predict_prob, cluster.predict(X_defender_scaled_cluster))))''' \n",
    "\n",
    "    y_pred = cluster.predict_proba(X_defender_scaled_cluster)\n",
    "    y_pred = y_pred * factor_of_reliability[i]\n",
    "    cluster_pred.append(y_pred)\n",
    "    i+=1\n",
    "all_fusion = []\n",
    "\n",
    "# Bayesian_fusion, sum of clusters probabilities \n",
    "all_fusion = []\n",
    "preds = sum(cluster_pred)\n",
    "# Normalization, for each prediction summed before, normalization in function of the class importance\n",
    "for pred in preds:\n",
    "    pred_array = np.array(pred)  # convert pred to a NumPy array\n",
    "    fusion = pred_array / np.sum(pred_array)  # perform normalization\n",
    "    all_fusion.append(fusion.tolist())  # append normalized values to all_fusion list \n",
    "\n",
    "guesses = []\n",
    "for i in range(len(all_fusion)):\n",
    "    if all_fusion[i][0]>=all_fusion[i][1]:\n",
    "        guesses.append(0) \n",
    "    else :    \n",
    "        guesses.append(1) \n",
    "        \n",
    "print(classification_report(y_true=y_test_defender, y_pred=guesses))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoteMaj Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VoteMaj    \n",
    "i=0\n",
    "cluster_pred = []\n",
    "for cluster in all_cluster:\n",
    "    X_defender_scaled_cluster = data_test[i]\n",
    "    # for DNN\n",
    "    '''scaler.fit(data_test[i].to_numpy())\n",
    "    X_defender_scaled_cluster = scaler.transform(data_test[i])\n",
    "    y_pred=cluster.predict(X_defender_scaled_cluster)\n",
    "    y_pred=np.argmax(y_pred,axis=1)''' \n",
    "        \n",
    "    y_pred = cluster.predict(X_defender_scaled_cluster) \n",
    "    cluster_pred.append(y_pred)\n",
    "    i+=1\n",
    "def MajorityVoteRule(pred_instance):\n",
    "    pred_instance.count(0)\n",
    "    if pred_instance.count(0)> pred_instance.count(1):\n",
    "        return 0\n",
    "    return 1\n",
    "guesses = []\n",
    "for k in range(len(data_test[0])):\n",
    "    pred_instance =[]\n",
    "    for j in range(len(cluster_pred)):\n",
    "        pred_instance.append(cluster_pred[j][k]) \n",
    "    guesses.append(MajorityVoteRule(pred_instance))\n",
    "print(classification_report(y_true=y_test_defender, y_pred=guesses))   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMP_SHAFER Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # DEMP_SHAFER\n",
    "def DempsterRule(m1,m2):\n",
    "    ## extract the frame of discernment      \n",
    "    sets=set(m1.keys()).union(set(m2.keys()))\n",
    "    result=dict.fromkeys(sets,0)\n",
    "    ## Combination process\n",
    "    for i in m1.keys():\n",
    "        for j in m2.keys():\n",
    "            if set(str(i)).intersection(set(str(j))) == set(str(i)):\n",
    "                result[i]+=m1[i]*m2[j]\n",
    "            elif set(str(i)).intersection(set(str(j))) == set(str(j)):\n",
    "                result[j]+=m1[i]*m2[j]      \n",
    "    ## normalize the results\n",
    "    f= sum(list(result.values()))\n",
    "    for i in result.keys():\n",
    "        result[i] /=f\n",
    "    return result\n",
    "def DempsterShaferEvidenceCombinationRule (ClassProbabilities, Weights):\n",
    "    MassFunctionList= []\n",
    "    for i in range(len(ClassProbabilities)):\n",
    "        MassFunction = {\"a\":Weights[i]*ClassProbabilities[i][0], \"b\":Weights[i]*ClassProbabilities[i][1],\n",
    "                        \"ab\":1-Weights[i]*ClassProbabilities[i][0]-Weights[i]*ClassProbabilities[i][1]}\n",
    "        MassFunctionList.append(MassFunction)\n",
    "    for i,mass in enumerate(MassFunctionList):\n",
    "        if i == 0:\n",
    "            DSresult = mass\n",
    "        else: \n",
    "            DSresult = DempsterRule(DSresult,mass)\n",
    "    if DSresult['a'] >= DSresult['b']:\n",
    "        return 0, DSresult['a']\n",
    "    else:\n",
    "        return 1, DSresult['b'] \n",
    "cluster_pred = []\n",
    "i = 0\n",
    "for cluster in all_cluster:\n",
    "    X_defender_scaled_cluster = data_test[i]\n",
    "    # For DNN\n",
    "    '''scaler.fit(data_test[i].to_numpy())\n",
    "    X_defender_scaled_cluster = scaler.transform(data_test[i])\n",
    "    def predict_prob(number):\n",
    "        return [number[0],1-number[0]]\n",
    "    y_pred = np.array(list(map(predict_prob, cluster.predict(X_defender_scaled_cluster))))'''\n",
    "    y_pred = cluster.predict_proba(X_defender_scaled_cluster)\n",
    "    cluster_pred.append(y_pred)\n",
    "    i+=1\n",
    "guesses = []    \n",
    "for s in range(len(data_test[0])):\n",
    "    loc = []\n",
    "    for n in range(len(data_test)):\n",
    "        loc.append(cluster_pred[n][s]) \n",
    "    guesses.append(DempsterShaferEvidenceCombinationRule(loc , factor_of_reliability ))\n",
    "guesses1 = []\n",
    "for i in range(len(data_test[0])):\n",
    "    guesses1.append(guesses[i][0])      \n",
    "print(classification_report(y_true=y_test_defender, y_pred=guesses1))   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
