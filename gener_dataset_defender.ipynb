{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42) # Used to always train de model in a same way to make assumptions\n",
    "from alive_progress import alive_bar\n",
    "import time\n",
    "from keras import backend as K\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "scaler = MinMaxScaler()\n",
    "categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model defender Loading\n",
    "#defender_model= tf.keras.models.load_model('./models_defender/dnndefender', custom_objects={'categorical_cross_entropy': categorical_cross_entropy})\n",
    "defender_model= pkl.load(open('./models_defender/rfdefender.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "defender_dataset= pd.read_csv('./dataset/Bot_IoT/defender_dataset.csv')\n",
    "defender_dataset=defender_dataset.drop(columns = ['attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert instance of defense to adv-ex\n",
    "Mat_masks=[]\n",
    "Mat_nb_of_steps=[]\n",
    "dur_p=[]\n",
    "spkets_p=[]\n",
    "sbytes_p=[]\n",
    "\n",
    "\n",
    "dataset_input = defender_dataset # From the defender\n",
    "model_input = defender_model\n",
    "# The mask combinaisons that can be adapted in function of the studied domain. Depends of the manipulable factors\n",
    "combinaisons = [ # Don't take [000] because not relevant, so 7 combinaisons. [001] = duration, [010] = spkts et [100]sbytes \n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [1,0,0],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,0],\n",
    "    [1,1,1]\n",
    "]\n",
    "# It's the max value of the attacker or defender dataset\n",
    "max_dur = dataset_input['dur'].max()\n",
    "max_pkts = dataset_input['spkts'].max()\n",
    "max_out = dataset_input['sbytes'].max()\n",
    "\n",
    "# Used to generate Adv Ex. Don't take the label to generate adv ex\n",
    "ben_dataset = dataset_input.loc[dataset_input['category'] == 0]\n",
    "mal_dataset = dataset_input.loc[dataset_input['category'] != 0]\n",
    "#mal_dataset, in_dataset = train_test_split(mal_dataset, random_state=42, stratify=mal_dataset.category, shuffle=True, train_size=(200/mal_dataset.shape[0]))\n",
    "\n",
    "# Mean determination to determine during peturbation generation the direction of the perturbation (negative or positive)\n",
    "ben_mean_dur = ben_dataset['dur'].mean()\n",
    "ben_mean_Spkts = ben_dataset['spkts'].mean()\n",
    "ben_mean_Sbytes = ben_dataset['sbytes'].mean()\n",
    "\n",
    "mal_dataset_reduced, in_dataset = train_test_split(mal_dataset, random_state=42, stratify=mal_dataset.category, shuffle=True, train_size=(100/mal_dataset.shape[0]))\n",
    "#mal_dataset_reduced=mal_dataset\n",
    "#vecteur=mal_dataset_reduced['category']\n",
    "mal_dataset_reduced=mal_dataset_reduced.drop(columns = ['category'])\n",
    "scaler.fit(mal_dataset_reduced.to_numpy())# to numpy to avoid the warning later when we predict with a numpy instead of dataframe\n",
    "\n",
    "adv_ex = []\n",
    "clean_ex=[]\n",
    "total_ex = []\n",
    "\n",
    "tot_nb_of_steps = [] # Used to know what the needed mean steps to create an adversarial example\n",
    "nb_of_needed_step = 0\n",
    "\n",
    "tot_masks = []# Used to know what the most used mask to create an adversarial example\n",
    "index_of_mask = 0\n",
    "\n",
    "start = time.process_time()\n",
    "with alive_bar(len(mal_dataset_reduced)) as bar:\n",
    "    # For each malicious instance\n",
    "    for index, row in mal_dataset_reduced.iterrows():\n",
    "        breaked = False\n",
    "        perturb_direction = []\n",
    "        \n",
    "        # Check the direction of perturbation for the 4 instance features\n",
    "        if(row[19] <= ben_mean_Sbytes): # Out\n",
    "            perturb_direction.append(1)\n",
    "        else:\n",
    "            perturb_direction.append(-1)\n",
    "            \n",
    "        if(row[17] <= ben_mean_Spkts): # spkts\n",
    "            perturb_direction.append(1)\n",
    "        else:\n",
    "            perturb_direction.append(-1)    \n",
    "            \n",
    "        if(row[14] <= ben_mean_dur): # dur\n",
    "            perturb_direction.append(1)\n",
    "        else:\n",
    "            perturb_direction.append(-1)\n",
    "\n",
    "        dif_mean_dur = ben_dataset[['dur']].mean() - row[14]\n",
    "        dif_mean_dur = abs(dif_mean_dur[0])\n",
    "        dif_mean_pkts = ben_dataset[['spkts']].mean() -row[17]\n",
    "        dif_mean_pkts = abs(dif_mean_pkts[0])\n",
    "        dif_mean_out = ben_dataset[['sbytes']].mean() - row[19]\n",
    "        dif_mean_out = abs(dif_mean_out[0])\n",
    "        \n",
    "        # Max 10 iterations of iterative perturbation to try to get benign instance\n",
    "        for i in range(1, 10):\n",
    "            nb_of_needed_step += 1 # start directly at the round 1\n",
    "            # Iterate while not benign \n",
    "            if(breaked==False):\n",
    "                # For each 7 combinations of perturbations\n",
    "                for combi in combinaisons:\n",
    "                    index_of_mask += 1 # check which mask is used\n",
    "                    # add perturbation to the autorized features                    \n",
    "                    adv = np.array(row)\n",
    "                    \n",
    "                    perturb1 = np.array(combi[0]) * ( dif_mean_out * (i*0.01) * perturb_direction[0])\n",
    "                    perturb2 = np.array(combi[1]) * (dif_mean_pkts * (i*0.01) * perturb_direction[1])\n",
    "                    perturb3 = np.array(combi[2]) * (dif_mean_dur * (i*0.01) * perturb_direction[2])\n",
    "\n",
    "                    # Addition of crafted perturbation\n",
    "                    adv[19] = adv[19] + perturb1 # sBytes\n",
    "                    adv[17] = adv[17] + perturb2 # sPackets\n",
    "                    adv[17] = int(adv[17] ) \n",
    "                    adv[14] = adv[14] + perturb3 # duration  # cast in INT to keep only the integer value\n",
    "\n",
    "                    # Syntactic Constraints\n",
    "                    # Add projection on the max value present in the dataset to keep the physical limitation\n",
    "                    if(adv[14] > max_dur):\n",
    "                        adv[14] = max_dur\n",
    "                    if(adv[19] > max_out):\n",
    "                        adv[19] = max_out\n",
    "                    if(adv[17] > max_pkts):\n",
    "                        adv[17] = max_pkts\n",
    "\n",
    "                    # if there is new bytes, normaly there is also at least 1 packet\n",
    "                    if(adv[17] == 0 and adv[19] > 0):\n",
    "                        adv[17] = 1 # Maybe change this part\n",
    "\n",
    "                    if(adv[19]/adv[17]>1500):\n",
    "                        n=int((adv[19]-row[19])/1500)\n",
    "                        adv[17]=adv[17]+n+1\n",
    "\n",
    "                    # Add the Semantic Contraints\n",
    "                    # Total number of Bytes in the communication. Sum of sbytes and InBytes feature values.\n",
    "                    adv[16] = adv[19]+adv[20] # TotBytes\n",
    "                    adv[15] = adv[17]+adv[18] # Totpakts\n",
    "\n",
    "                    # Average number of bytes exchanged per packet. Ratio between TotBytes and spkts.\n",
    "                    adv[21] = adv[15]/adv[14] # raet\n",
    "                    # Average number of bytes exchanged per second. Ratio between TotBytes and duration.\n",
    "                    adv[22] = adv[17]/adv[14] # Sraet\n",
    "                    # Average number of packets exchanged per second. Ratio between spkts and duration.\n",
    "                    adv[23] = adv[18]/adv[14] # Draet\n",
    "                        \n",
    "                    adv2 = [] # used to fit with the input of the model because normaly take a matrix, so need the matrix notation, even for a vector\n",
    "                    adv2.append(adv)\n",
    "                    #For DNN\n",
    "                    '''adv2= pd.DataFrame(adv2)\n",
    "                    scaler.fit(adv2.to_numpy())\n",
    "                    adv2_scaled = scaler.transform(adv2) # For DNN\n",
    "                    test = model_input.predict(adv2_scaled)\n",
    "                    test = np.argmax(test,1) # For DNN '''\n",
    "                    test = model_input.predict(adv2) # For other model than DNN\n",
    "                    if (test == 0): # benign break\n",
    "                        dur_p.append(abs(perturb3))\n",
    "                        spkets_p.append(abs(perturb2))\n",
    "                        sbytes_p.append(abs(perturb1))\n",
    "                        adv_ex.append(adv) # adv_ex contains all adversarial examples that fool the classifier\n",
    "                        clean_ex.append(row)\n",
    "                        breaked = True\n",
    "                        tot_masks.append(index_of_mask)\n",
    "                        tot_nb_of_steps.append(nb_of_needed_step)\n",
    "                        break                     \n",
    "            index_of_mask = 0\n",
    "        nb_of_needed_step = 0  \n",
    "        \n",
    "        total_ex.append(adv) # Total adversarial examples. append the final created adv ex that fool or not \n",
    "        bar()\n",
    "end = time.process_time()-start\n",
    "\n",
    "data_set_defense_adv=pd.DataFrame(adv_ex)\n",
    "data_set_defense_adv.columns=['protoudp', 'dportWell-known', 'dportUnspecified', 'dportRegistered',\n",
    "    'dportDynamic/Private', 'sportWell-known', 'sportUnspecified',\n",
    "    'sportRegistered', 'sportDynamic/Private', 'prototcp', 'protorarp',\n",
    "    'protoigmp', 'protoicmp', 'protoarp', 'dur', 'pkts', 'bytes', 'spkts',\n",
    "    'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate']\n",
    "data_set_defense_adv['label']= 1\n",
    "\n",
    "data_set_defense_clean=pd.DataFrame(clean_ex)\n",
    "data_set_defense_clean.columns=['protoudp', 'dportWell-known', 'dportUnspecified', 'dportRegistered',\n",
    "    'dportDynamic/Private', 'sportWell-known', 'sportUnspecified',\n",
    "    'sportRegistered', 'sportDynamic/Private', 'prototcp', 'protorarp',\n",
    "    'protoigmp', 'protoicmp', 'protoarp', 'dur', 'pkts', 'bytes', 'spkts',\n",
    "    'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate']\n",
    "data_set_defense_clean['label']= 0\n",
    "\n",
    "data_defense_defender=pd.concat([data_set_defense_adv, data_set_defense_clean], ignore_index = True)\n",
    "data_defense_defender.to_csv('./dataset/Data_defense/data_defense_defender1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
